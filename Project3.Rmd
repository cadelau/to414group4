---
title: "Project3"
author: "Naman Gupta"
date: "Monday, April 20, 2020"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Online shopping has grown significatly in the last decade. Massive online sites like Amazon and Walmart are the go-to shopping place now rather than brick and mortar stores for some people. With coronavirus precautions still in effect, people are now unable to even go to physical store locations, driving the need for online shopping even higher. 

Our data deals with customer data when they are on a store's website. The variable of interest is called Revenue, and identifies whether a customer actually bought something while on the site. Not surprisingly, online retail sites have very low rates of people making purchases. With how easy it is to simply get online and browse, we see this as the equivalent of most people "window shopping" online. That is, they are merely there to look, not necessarily buy.

We think that by using machine learning we can identify what factors result in customers actually buying products online. The insights to be drawn could be extremely valuable to the online retail company by improving buy-rates. Further analyses of insights will be given later.

# Data Exploration and Preprocessing

As previously mentioned, the data comes from online shopping sessions, and whether a customer actually bought something during their time on the site. The data has over 12,000 observations and 18 variables. There are columns describing what types of pages the customer visits(administrative, informational, product related), as well as how long they stay on each of those pages. There is information on the customer such as what browser and operating system they are using, and whether or not they are a returning customer. Not all of the data will be necessary, and some of it must be processed further before being useful. We will detail those changes in this section. 

```{r}
library(caret)
# Set the same seed
set.seed(123)
shopping <- read.csv("online_shoppers_intention.csv")

shopping$Month <- NULL
shopping$OperatingSystems <- NULL
shopping$Browser <- NULL
#shopping$Region <- as.factor(shopping$Region)
shopping$Region <- NULL
shopping$TrafficType <- NULL
shopping$Weekend <- as.factor(shopping$Weekend)
shopping$Revenue <- as.factor(shopping$Revenue)
```

* Altered columns:
    + **Month**: Removed for now. Could change to a "holiday vs. non holiday" season variable.
    + **Operating System**: Removed. Should not affect whether someone buys or not.
    + **Browser**: Removed for same reason as operating system.
    + **Region**: Removed. Not interested in regional analysis for now.
    + **Traffic Type**: Removed. Unclear what different values in this column represent.
    + **Weekend**: Changed to factor.
    + **Revenue**: Changed to factor.

```{r}
# Normalize data
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
data_to_norm <- shopping[,1:10]
str(shopping)
shop_norm <- as.data.frame(lapply(data_to_norm, normalize))
#shop_norm$Region <- shopping$Region
shop_norm$VisitorType <- shopping$VisitorType
shop_norm$Weekend <- shopping$Weekend
shop_norm$Revenue <- shopping$Revenue


# Expand factors into dummy variables
y <- shop_norm$Revenue
shop_norm$Revenue <- NULL
X = as.data.frame(model.matrix( ~ .^2 - 1, shop_norm))
X$Revenue <- as.factor(y)
```

* Split data into train and test.
* Logistic model on train data

```{r}
rows <- sample(nrow(X))
shop <- X[rows,]

# This is roughly a 70/30 train/test split
# Only use the train data to tune and improve models right now
shopping_train <- shop[1:8630,]
shopping_test <- shop[8631:12330,]

# Logistic Model
log_model <- glm(Revenue ~ ., data=shopping_train, family="binomial")
# summary(log_model)

f = "Revenue ~ ProductRelated_Duration + PageValues + WeekendTRUE + Administrative:PageValues + Informational:PageValues + Informational:VisitorTypeReturning_Visitor + Informational_Duration:SpecialDay + ProductRelated:ProductRelated_Duration + ProductRelated:PageValues + BounceRates:PageValues + ExitRates:PageValues + ExitRates:VisitorTypeReturning_Visitor + PageValues:SpecialDay + PageValues:WeekendTRUE + VisitorTypeReturning_Visitor:WeekendTRUE"
```

# Models

### SVM Model

```{r}
library(kernlab)
library(e1071)

tuned = tune.svm(Revenue ~ ProductRelated_Duration + PageValues + WeekendTRUE + Administrative:PageValues + Informational:PageValues + Informational:VisitorTypeReturning_Visitor + Informational_Duration:SpecialDay + ProductRelated:ProductRelated_Duration + ProductRelated:PageValues + BounceRates:PageValues + ExitRates:PageValues + ExitRates:VisitorTypeReturning_Visitor + PageValues:SpecialDay + PageValues:WeekendTRUE + VisitorTypeReturning_Visitor:WeekendTRUE, data = shopping_train, class.weights=c("FALSE"=0.44, "TRUE"=1), tunecontrol=tune.control(cross=10))
# summary(tuned)
svmfit = tuned$best.model
# table(data_test[,c("Revenue")], predict(svmfit, data_test, type = "response"))

svm_predictions <- predict(svmfit, shopping_test)
confusionMatrix(as.factor(svm_predictions), shopping_test$Revenue)
```

### kNN Model

```{r}
trControl <- trainControl(method  = "cv",
                          number  = 10)

fit <- train(Revenue ~ ProductRelated_Duration + PageValues + WeekendTRUE + Administrative:PageValues + Informational:PageValues + Informational:VisitorTypeReturning_Visitor + Informational_Duration:SpecialDay + ProductRelated:ProductRelated_Duration + ProductRelated:PageValues + BounceRates:PageValues + ExitRates:PageValues + ExitRates:VisitorTypeReturning_Visitor + PageValues:SpecialDay + PageValues:WeekendTRUE + VisitorTypeReturning_Visitor:WeekendTRUE,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:25),
             trControl  = trControl,
             metric     = "Kappa",
             data       = shopping_train)

# fit

knn_predictions <- predict(fit, shopping_test)
confusionMatrix(as.factor(knn_predictions), shopping_test$Revenue)
```

### Decision Tree Model

```{r}
library(C50)
library(partykit)
index = which(colnames(shopping_train)=="Revenue")
folds = createFolds(shopping_train$Revenue, 10)

dt_cv1 = lapply(folds, function(x) {
  training_fold <- shopping_train[-x,]
  test_fold <- shopping_train[x,]
  
  error_cost <- matrix(c(0, 1, 4, 0), nrow = 2)
  
  clf <- C5.0(Revenue ~ ProductRelated_Duration + PageValues + WeekendTRUE + Administrative:PageValues + Informational:PageValues + Informational:VisitorTypeReturning_Visitor + Informational_Duration:SpecialDay + ProductRelated:ProductRelated_Duration + ProductRelated:PageValues + BounceRates:PageValues + ExitRates:PageValues + ExitRates:VisitorTypeReturning_Visitor + PageValues:SpecialDay + PageValues:WeekendTRUE + VisitorTypeReturning_Visitor:WeekendTRUE,
              data = training_fold,
              costs = error_cost)
  
  y_pred <- predict(clf, newdata = test_fold[,-index])
  cm = table(test_fold$Revenue, y_pred)
  # print(cm)
  observed_acc <- cm[1,1] + cm[2,2]
  expected_acc <- ((cm[1,1]+cm[1,2])*(cm[1,1]+cm[2,1]) + ((cm[2,1]+cm[2,2])*(cm[1,2]+cm[2,2])))/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
  kappa <- (observed_acc - expected_acc)/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2] - expected_acc)
  balanced_accuracy <- 0.5*(cm[1,1]/(cm[1,1]+cm[1,2]) + cm[2,2]/(cm[2,2]+cm[2,1]))
  # print(balanced_accuracy)
  return(kappa)
})
dt1_avg_kappa <- mean(as.numeric(dt_cv1))
dt1_avg_kappa

decision_tree_model <- C5.0(Revenue ~ ProductRelated_Duration + PageValues + WeekendTRUE + Administrative:PageValues + Informational:PageValues + Informational:VisitorTypeReturning_Visitor + Informational_Duration:SpecialDay + ProductRelated:ProductRelated_Duration + ProductRelated:PageValues + BounceRates:PageValues + ExitRates:PageValues + ExitRates:VisitorTypeReturning_Visitor + PageValues:SpecialDay + PageValues:WeekendTRUE + VisitorTypeReturning_Visitor:WeekendTRUE,
              data = shopping_train,
              costs = matrix(c(0, 1, 4, 0), nrow = 2))

decision_tree_predictions <- predict(decision_tree_model, shopping_test)
confusionMatrix(as.factor(decision_tree_predictions), shopping_test$Revenue)
```

### Stacked Model

```{r}
# ==========================================================
# Get the various predictions for the train data
# ==========================================================
svm_predictions_train <- predict(svmfit, shopping_train)
knn_predictions_train <- predict(fit, shopping_train)
decision_tree_predictions_train <- predict(decision_tree_model, shopping_train)

ConvertToTF <- function(myprediction) {
  result <- as.factor(myprediction)
  levels(result) <- c("FALSE", "TRUE")
  result
}

# Convert each set of prediction to factors
svm_predictions <- ConvertToTF(svm_predictions)
knn_predictions <- ConvertToTF(knn_predictions)
decision_tree_predictions <- ConvertToTF(decision_tree_predictions)

# Create the feature data
stacked_data <- data.frame(svm_predictions,knn_predictions, decision_tree_predictions)

model_combined_results <- data.frame(svm_predictions_train, knn_predictions_train, decision_tree_predictions_train)

names(model_combined_results) <- names(stacked_data)

# Create the model
stacked_model <- ctree(shopping_train$Revenue ~ . + 1, data = model_combined_results)
plot(stacked_model)

stacked_model_prediction <- predict(stacked_model, stacked_data, type = "response")

stacked_model_prediction <- ConvertToTF(stacked_model_prediction)
confusionMatrix(as.factor(stacked_model_prediction), shopping_test$Revenue)
```

# Conclusion







